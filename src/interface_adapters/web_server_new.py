"""
Flask web server with Apple-style UI for TOPIS notice crawler.
Refactored version with separated templates and static files.
"""
import asyncio
import json
import os
import tempfile
from datetime import datetime
from flask import Flask, request, jsonify, send_file, render_template, Response
from flask_cors import CORS

import sys
import os

# Add project root to Python path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.application.notice_crawler_service import NoticeCrawlerService
from src.application.multi_site_crawler_service import MultiSiteCrawlerService
from src.application.word_export_service import WordExportService
from src.domain.notice import NoticeCategory, Site, SiteConfigManager
from src.domain.date_filter import DateFilter
from src.infrastructure.selenium_notice_repository import SeleniumNoticeRepository

# Flask ì•± ì„¤ì • - í…œí”Œë¦¿ê³¼ ì •ì  íŒŒì¼ ê²½ë¡œ ì§€ì •
template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')
static_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static')

app = Flask(__name__, template_folder=template_dir, static_folder=static_dir)
CORS(app)

# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
multi_site_service = None
legacy_crawler_service = None  # ê¸°ì¡´ í˜¸í™˜ì„±ìš©
word_service = WordExportService()

# ê³µì§€ì‚¬í•­ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜) - ì‚¬ì´íŠ¸ë³„ë¡œ ê´€ë¦¬
notice_cache = {}  # {site_notice_id: notice_data}


def get_multi_site_service():
    """ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°."""
    global multi_site_service
    if multi_site_service is None:
        multi_site_service = MultiSiteCrawlerService()
    return multi_site_service


def get_crawler_service():
    """í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í˜¸í™˜ì„±)."""
    global legacy_crawler_service
    if legacy_crawler_service is None:
        repository = SeleniumNoticeRepository()
        legacy_crawler_service = NoticeCrawlerService(repository)
    return legacy_crawler_service


def parse_category(category_str: str) -> NoticeCategory:
    """ë¬¸ìì—´ì„ ì¹´í…Œê³ ë¦¬ enumìœ¼ë¡œ ë³€í™˜."""
    category_map = {
        'traffic': NoticeCategory.TRAFFIC_CONTROL,
        'bus': NoticeCategory.BUS,
        'policy': NoticeCategory.POLICY, 
        'weather': NoticeCategory.WEATHER,
        'etc': NoticeCategory.ETC,
        'all': NoticeCategory.ALL
    }
    return category_map.get(category_str, NoticeCategory.ALL)


def parse_site(site_str: str) -> Site:
    """ë¬¸ìì—´ì„ ì‚¬ì´íŠ¸ enumìœ¼ë¡œ ë³€í™˜."""
    site_map = {
        'topis': Site.TOPIS,
        'ictr': Site.ICTR
    }
    return site_map.get(site_str, Site.TOPIS)


@app.route('/favicon.ico')
def favicon():
    """Favicon í•¸ë“¤ëŸ¬ - ë¸Œë¼ìš°ì € 404 ì˜¤ë¥˜ ë°©ì§€."""
    favicon_data = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89\x00\x00\x00\nIDATx\x9cc\x00\x01\x00\x00\x05\x00\x01\r\n-\xdb\x00\x00\x00\x00IEND\xaeB`\x82'
    return Response(favicon_data, mimetype='image/png')


@app.route('/api/sites')
def get_sites():
    """ì§€ì›í•˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ê³¼ ì„¤ì • ì¡°íšŒ API."""
    try:
        service = get_multi_site_service()
        configs = service.get_site_configs()
        
        return jsonify({
            'success': True,
            'sites': configs
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500


@app.route('/')
def index():
    """ë©”ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤ - ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ UI."""
    return render_template('index.html')


@app.route('/api/crawl', methods=['POST'])
def crawl_notices():
    """ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ API."""
    try:
        data = request.get_json()
        
        category_str = data.get('category', 'all')
        start_date_str = data.get('start_date')
        end_date_str = data.get('end_date')
        max_pages = data.get('max_pages', 3)
        with_content = data.get('with_content', True)
        
        # ì¹´í…Œê³ ë¦¬ ë³€í™˜
        category = parse_category(category_str)
        
        # ë‚ ì§œ í•„í„° ìƒì„±
        start_date = None
        end_date = None
        
        if start_date_str:
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
        if end_date_str:
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
            
        date_filter = DateFilter(start_date=start_date, end_date=end_date)
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        service = get_crawler_service()
        
        print(f"ğŸ” DEBUG - ì „ì²´ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§:")
        print(f"   ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print(f"   ì‹œì‘ ë‚ ì§œ: {start_date}")
        print(f"   ì¢…ë£Œ ë‚ ì§œ: {end_date}")
        
        # ğŸš€ í•­ìƒ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ì€ UIì—ì„œë§Œ)
        from src.domain.notice import NoticeCategory
        all_notices = asyncio.run(service.crawl_category_fast(NoticeCategory.ALL, max_pages=max_pages))
        print(f"ğŸ” DEBUG - ë¹ ë¥¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_notices)}ê°œ")
        
        # Notice ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        notices = []
        for notice in all_notices:
            notice_dict = {
                'id': notice.id,
                'title': notice.title,
                'category': service._get_category_name(notice.category),
                'created_date': notice.created_date.isoformat(),
                'view_count': notice.view_count,
                'has_attachment': notice.has_attachment,
                'content': notice.content  # ë¹ ë¥¸ í¬ë¡¤ë§ì´ë¯€ë¡œ null
            }
            notices.append(notice_dict)
        
        print(f"ğŸ” DEBUG - ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì™„ë£Œ: {len(notices)}ê°œ (contentëŠ” Word ë‹¤ìš´ë¡œë“œ ì‹œ ìë™ ì¶”ê°€ë¨)")
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            print(f"ğŸ” DEBUG - ë‚ ì§œ í•„í„°ë§ ì‹œì‘:")
            print(f"   í•„í„° ë²”ìœ„: {date_filter}")
            print(f"   í•„í„°ë§ ì „: {len(notices)}ê°œ")
            
            filtered_notices = []
            for notice in notices:
                # ISO í˜•íƒœ ì§ì ‘ íŒŒì‹±
                notice_date = datetime.fromisoformat(notice['created_date'])
                is_valid = date_filter.is_in_range(notice_date)
                status = "âœ… í¬í•¨" if is_valid else "âŒ ì œì™¸"
                print(f"   {notice['id']} | {notice_date.strftime('%Y-%m-%d')} | {status}")
                
                if is_valid:
                    filtered_notices.append(notice)
            
            notices = filtered_notices
            print(f"ğŸ” DEBUG - í•„í„°ë§ ì™„ë£Œ: {len(notices)}ê°œ")
        else:
            print(f"ğŸ” DEBUG - ë‚ ì§œ í•„í„°ë§ ì—†ìŒ: {len(notices)}ê°œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        
        # âš¡ ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥ (ë¹ ë¥¸ Word ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´)
        global notice_cache
        for notice in notices:
            notice_cache[notice['id']] = notice
        
        print(f"ğŸ” DEBUG - ìµœì¢… ê²°ê³¼: {len(notices)}ê°œ ê³µì§€ì‚¬í•­ì„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ì†¡")
        
        # ì„ì‹œë¡œ notices ì €ì¥ (Word ë‚´ë³´ë‚´ê¸°ì—ì„œ ì‚¬ìš©)
        temp_file = tempfile.mktemp(suffix='.json')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(notices, f, ensure_ascii=False, indent=2)
        
        # ì„¸ì…˜ì— ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥ (ì‹¤ì œë¡œëŠ” Redisë‚˜ DB ì‚¬ìš© ê¶Œì¥)
        app.config['LAST_CRAWL_FILE'] = temp_file
        
        response = jsonify({
            'success': True,
            'count': len(notices),
            'notices': notices,
            'period': str(date_filter)
        })
        
        # ìºì‹œ ë°©ì§€ í—¤ë” ì¶”ê°€
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        
        return response
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500


@app.route('/api/crawl-multi', methods=['POST'])
def crawl_multi_site():
    """ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ API."""
    try:
        data = request.get_json()
        
        site_str = data.get('site', 'topis')
        start_date_str = data.get('start_date')
        end_date_str = data.get('end_date')
        max_pages = data.get('max_pages', 3)
        with_content = data.get('with_content', False)
        
        # ì‚¬ì´íŠ¸ ë³€í™˜
        site = parse_site(site_str)
        
        # ë‚ ì§œ í•„í„° ìƒì„±
        start_date = None
        end_date = None
        
        if start_date_str:
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
        if end_date_str:
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
            
        date_filter = DateFilter(start_date=start_date, end_date=end_date)
        
        # ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ì„œë¹„ìŠ¤ ì‚¬ìš©
        service = get_multi_site_service()
        
        print(f"ğŸŒ DEBUG - ë‹¤ì¤‘ ì‚¬ì´íŠ¸ í¬ë¡¤ë§:")
        print(f"   ì‚¬ì´íŠ¸: {site.value}")
        print(f"   ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print(f"   ì‹œì‘ ë‚ ì§œ: {start_date}")
        print(f"   ì¢…ë£Œ ë‚ ì§œ: {end_date}")
        
        notices = []
        
        if site == Site.TOPIS:
            # TOPIS í¬ë¡¤ë§
            category_str = data.get('category', 'all')
            category = parse_category(category_str)
            notices = asyncio.run(service.crawl_site(
                site=site,
                category=category,
                max_pages=max_pages,
                per_page=10
            ))
        elif site == Site.ICTR:
            # ICTR í¬ë¡¤ë§ (ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨)
            keyword = data.get('keyword', '')
            search_type = data.get('search_type', 'title')
            search_params = {
                'keyword': keyword,
                'search_type': search_type
            } if keyword else None
            
            notices = asyncio.run(service.crawl_site(
                site=site,
                max_pages=max_pages,
                per_page=10,
                search_params=search_params
            ))
            
        print(f"ğŸŒ DEBUG - í¬ë¡¤ë§ ì™„ë£Œ: {len(notices)}ê°œ")
        
        # Notice ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        notices_dicts = []
        for notice in notices:
            notice_dict = {
                'id': notice.id,
                'title': notice.title,
                'category': service._get_category_name(notice.category),
                'created_date': notice.created_date.isoformat(),
                'view_count': notice.view_count,
                'has_attachment': notice.has_attachment,
                'site': notice.site.value,
                'department': notice.department,
                'content': notice.content
            }
            notices_dicts.append(notice_dict)
        
        print(f"ğŸŒ DEBUG - ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì™„ë£Œ: {len(notices_dicts)}ê°œ")
        
        # ë‚ ì§œ í•„í„°ë§
        if start_date or end_date:
            print(f"ğŸŒ DEBUG - ë‚ ì§œ í•„í„°ë§ ì‹œì‘:")
            print(f"   í•„í„° ë²”ìœ„: {date_filter}")
            print(f"   í•„í„°ë§ ì „: {len(notices_dicts)}ê°œ")
            
            filtered_notices = []
            for notice in notices_dicts:
                notice_date = datetime.fromisoformat(notice['created_date'])
                is_valid = date_filter.is_in_range(notice_date)
                status = "âœ… í¬í•¨" if is_valid else "âŒ ì œì™¸"
                print(f"   {notice['id']} | {notice_date.strftime('%Y-%m-%d')} | {status}")
                
                if is_valid:
                    filtered_notices.append(notice)
            
            notices_dicts = filtered_notices
            print(f"ğŸŒ DEBUG - í•„í„°ë§ ì™„ë£Œ: {len(notices_dicts)}ê°œ")
        else:
            print(f"ğŸŒ DEBUG - ë‚ ì§œ í•„í„°ë§ ì—†ìŒ: {len(notices_dicts)}ê°œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        
        # ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥ (ì‚¬ì´íŠ¸ ì •ë³´ í¬í•¨)
        global notice_cache
        for notice in notices_dicts:
            cache_key = f"{notice['site']}_{notice['id']}"
            notice_cache[cache_key] = notice
        
        print(f"ğŸŒ DEBUG - ìµœì¢… ê²°ê³¼: {len(notices_dicts)}ê°œ ê³µì§€ì‚¬í•­ì„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ì†¡")
        
        # ì„ì‹œë¡œ notices ì €ì¥ (Word ë‚´ë³´ë‚´ê¸°ì—ì„œ ì‚¬ìš©)
        temp_file = tempfile.mktemp(suffix='.json')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(notices_dicts, f, ensure_ascii=False, indent=2)
        
        # ì„¸ì…˜ì— ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì €ì¥
        app.config['LAST_CRAWL_FILE'] = temp_file
        
        response = jsonify({
            'success': True,
            'count': len(notices_dicts),
            'notices': notices_dicts,
            'period': str(date_filter),
            'site': site.value
        })
        
        # ìºì‹œ ë°©ì§€ í—¤ë” ì¶”ê°€
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        
        return response
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500


@app.route('/api/export/word/<notice_id>')
def export_word(notice_id):
    """Word ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ API."""
    try:
        if notice_id == 'all':
            # ì „ì²´ ê³µì§€ì‚¬í•­ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ
            temp_file = app.config.get('LAST_CRAWL_FILE')
            if not temp_file or not os.path.exists(temp_file):
                return jsonify({'error': 'ë¨¼ì € í¬ë¡¤ë§ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.'}), 400
            
            with open(temp_file, 'r', encoding='utf-8') as f:
                notices = json.load(f)
            
            if not notices:
                return jsonify({'error': 'ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400
            
            # Word ë¬¸ì„œ ìƒì„±
            output_path = tempfile.mktemp(suffix='.docx')
            word_service.create_multiple_notices_document(notices, output_path)
            
            return send_file(
                output_path,
                as_attachment=True,
                download_name=f"TOPIS_ê³µì§€ì‚¬í•­_ëª¨ìŒ_{datetime.now().strftime('%Y%m%d')}.docx",
                mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            )
        else:
            # âš¡ íŠ¹ì • ê³µì§€ì‚¬í•­ - ìºì‹œ ë¨¼ì € í™•ì¸
            global notice_cache
            notice_dict = None
            
            # notice_id í˜•ì‹: "site_id" (ì˜ˆ: "topis_5284", "ictr_1392")
            if '_' in notice_id:
                site_str, actual_id = notice_id.split('_', 1)
                site = parse_site(site_str)
            else:
                # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ë³¸ê°’ì€ TOPIS
                site = Site.TOPIS
                actual_id = notice_id
            
            # 1ë‹¨ê³„: ìºì‹œì—ì„œ í™•ì¸ (ì¦‰ì‹œ)
            cache_key = f"{site.value}_{actual_id}"
            if cache_key in notice_cache:
                cached_notice = notice_cache[cache_key]
                print(f"âš¡ CACHE HIT: Found cached data for notice {cache_key}")
                
                # ìºì‹œëœ ë°ì´í„°ì— contentê°€ ìˆëŠ”ì§€ í™•ì¸
                if cached_notice.get('content') and cached_notice['content'] not in [None, '', 'null']:
                    print(f"ğŸ“„ CONTENT AVAILABLE: Using cached content")
                    notice_dict = cached_notice
                else:
                    print(f"ğŸ“„ CONTENT MISSING: Fetching detailed content for Word export")
                    service = get_multi_site_service()
                    notice_dict = asyncio.run(service.get_notice_with_content(site, actual_id))
                    
                    # ìƒì„¸ ë‚´ìš©ì„ ìºì‹œì— ì—…ë°ì´íŠ¸
                    if notice_dict and notice_dict.get('content'):
                        notice_cache[cache_key] = notice_dict
                        print(f"ğŸ’¾ CACHE UPDATED: Content added to cache")
            
            # 2ë‹¨ê³„: ìºì‹œì— ì—†ìœ¼ë©´ í¬ë¡¤ë§ (ëŠë¦¼)
            if not notice_dict:
                print(f"ğŸ” CACHE MISS: Crawling notice {cache_key}")
                service = get_multi_site_service()
                notice_dict = asyncio.run(service.get_notice_with_content(site, actual_id))
                
                # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
                if notice_dict:
                    notice_cache[cache_key] = notice_dict
            
            if not notice_dict:
                return jsonify({'error': 'ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}), 404
            
            # Word ë¬¸ì„œ ìƒì„±
            output_path = tempfile.mktemp(suffix='.docx')
            word_service.create_notice_document(notice_dict, output_path)
            
            # íŒŒì¼ëª… ìƒì„± (ì‚¬ì´íŠ¸ë³„)
            safe_title = notice_dict['title'][:50]  # ì œëª© ê¸¸ì´ ì œí•œ
            safe_title = "".join(c for c in safe_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            site_name = "ì¸ì²œêµí†µê³µì‚¬" if site == Site.ICTR else "TOPIS"
            filename = f"{site_name}_{safe_title}_{datetime.now().strftime('%Y%m%d')}.docx"
            
            return send_file(
                output_path,
                as_attachment=True,
                download_name=filename,
                mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            )
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500


def run_server(host='127.0.0.1', port=8080, debug=False):
    """ì›¹ ì„œë²„ ì‹¤í–‰."""
    print("ğŸŒ TOPIS í¬ë¡¤ëŸ¬ ì›¹ ì„œë²„ê°€ ì‹œì‘ë©ë‹ˆë‹¤!")
    print(f"ğŸ”— ë¸Œë¼ìš°ì €ì—ì„œ http://{host}:{port} ë¥¼ ì—´ì–´ì£¼ì„¸ìš”.")
    print("ğŸ›‘ ì„œë²„ë¥¼ ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    
    app.run(host=host, port=port, debug=debug)


if __name__ == '__main__':
    run_server()
